{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/eric/data/social_images/dataset/vscocam/2019-01-02_11-43-37_UTC.txt\\tthis year be creative\\tportocool||porto||oporto||oportocity||portolovers||oportolovers||portonoinsta||igersporto||portugal||visitportugal||travel||travelgram||instatravel||travelphotography||vscocam||coolcities||citieswelove\\n', '/home/eric/data/social_images/dataset/vscocam/2018-12-18_03-06-21_UTC.txt\\tjust couple gala tryna christmas shop\\tvscocam\\n']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tkinter import _flatten\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "root_path='/home/eric/data/social_images'\n",
    "data_path=os.path.join(root_path,'datasets.txt')\n",
    "with open(data_path,'r') as f:\n",
    "    examples=f.readlines()\n",
    "print(examples[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58469/58469 [00:00<00:00, 161528.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['portocool', 'porto', 'oporto', 'oportocity', 'portolovers']\n",
      "[('love', 14837), ('instagood', 13534), ('photooftheday', 10932), ('photography', 8075), ('fashion', 8044)]\n",
      "['love', 'instagood', 'photooftheday', 'photography', 'fashion']\n"
     ]
    }
   ],
   "source": [
    "hashtags=[]\n",
    "for example in tqdm(examples):\n",
    "    arr=example.strip().split('\\t')\n",
    "    if(len(arr)<3):\n",
    "        print(arr[0])\n",
    "        continue\n",
    "    txt_path=arr[0]\n",
    "    tweets=arr[1]\n",
    "    hashtag=arr[2].split('||')\n",
    "    hashtags.append(hashtag)\n",
    "list_hashtag=list(_flatten(hashtags))\n",
    "print(list_hashtag[:5])\n",
    "count_hashtag=collections.Counter(list_hashtag)\n",
    "word_freq=count_hashtag.most_common(1000)\n",
    "print(word_freq[:5])\n",
    "words=[]\n",
    "for word in word_freq:\n",
    "    words.append(word[0])\n",
    "print(words[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import string\n",
    "nltk_stopw = stopwords.words('english')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "def tokenize (text):\n",
    "    tokens = [word.strip(string.punctuation) for word in RegexpTokenizer(r'\\b[a-zA-Z][a-zA-Z0-9]{1,50}\\b').tokenize(text)]\n",
    "    return tokens\n",
    " \n",
    "def removeStopWords (tokens):\n",
    "    filteredTokens = [f.lower() for f in tokens if f and f.lower() not in nltk_stopw]\n",
    "    return filteredTokens\n",
    " \n",
    "def stem (filteredTokens):      # stemmed & > 2 letters\n",
    "    return [stemmer.stem(token) for token in filteredTokens if len(token) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58469/58469 [00:27<00:00, 2145.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image tweets data finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "output=open('/home/eric/Documents/Hashtag-recommendation-for-social-images/image_text_hashtagging/datasets/image_text/image_text_data.txt','w')\n",
    "for example in tqdm(examples):\n",
    "    arr=example.strip().split('\\t')\n",
    "    if(len(arr)<3):\n",
    "        print(arr[0])\n",
    "        continue\n",
    "    txt_path=arr[0]\n",
    "    tweets=arr[1]\n",
    "    hashtag=arr[2].split('||')\n",
    "    filter_hashtag=[]\n",
    "    tweet_arr=tweets.split()  # remove stop words\n",
    "    tweet_arr=removeStopWords(tweet_arr)\n",
    "    tweet_arr=stem(tweet_arr)\n",
    "    tweets=' '.join(tweet_arr)\n",
    "    if not tweets:\n",
    "        continue\n",
    "    for item in hashtag:\n",
    "        if((item in words) and (item not in filter_hashtag)):\n",
    "            filter_hashtag.append(item)\n",
    "    if(filter_hashtag):\n",
    "#         print(filter_hashtag)\n",
    "        image_path=txt_path.strip().split('social_images')[1][1:]\n",
    "        img_loc=image_path.replace('txt','jpg')\n",
    "        output.write(img_loc+'*')\n",
    "        output.write(tweets+'*')\n",
    "        output.write(' '.join(filter_hashtag)+'\\n')\n",
    "output.close()\n",
    "print('image tweets data finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58469/58469 [00:27<00:00, 2144.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image data finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "root_path='/home/eric/Documents/Hashtag-recommendation-for-social-images/image_text_hashtagging/datasets/image'\n",
    "output=open(os.path.join(root_path,'image_data.txt'),'w')\n",
    "for example in tqdm(examples):\n",
    "    arr=example.strip().split('\\t')\n",
    "    if(len(arr)<3):\n",
    "        print(arr[0])\n",
    "        continue\n",
    "    txt_path=arr[0]\n",
    "    tweets=arr[1]\n",
    "    hashtag=arr[2].split('||')\n",
    "    filter_hashtag=[]\n",
    "    tweet_arr=tweets.split()  # remove stop words\n",
    "    tweet_arr=removeStopWords(tweet_arr)\n",
    "    tweet_arr=stem(tweet_arr)\n",
    "    tweets=' '.join(tweet_arr)\n",
    "    if not tweets:\n",
    "        continue\n",
    "    for item in hashtag:\n",
    "        if(item in words and (item not in filter_hashtag)):\n",
    "            filter_hashtag.append(item)\n",
    "    if(filter_hashtag):\n",
    "#         print(filter_hashtag)\n",
    "        image_path=txt_path.strip().split('social_images')[1][1:]\n",
    "        img_loc=image_path.replace('txt','jpg')\n",
    "        output.write(img_loc+'*')\n",
    "        output.write(' '.join(filter_hashtag)+'\\n')\n",
    "output.close()\n",
    "print('image data finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58469/58469 [00:27<00:00, 2149.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline image tweets data finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "root_path='/home/eric/Documents/Hashtag-recommendation-for-social-images/baselines/co-attention/data'\n",
    "output=open(os.path.join(root_path,'image_text_data.txt'),'w')\n",
    "num=0\n",
    "for example in tqdm(examples):\n",
    "    arr=example.strip().split('\\t')\n",
    "    if(len(arr)<3):\n",
    "        print(arr[0])\n",
    "        continue\n",
    "    txt_path=arr[0]\n",
    "    tweets=arr[1]\n",
    "    hashtag=arr[2].split('||')\n",
    "    filter_hashtag=[]\n",
    "    tweet_arr=tweets.split()  # remove stop words\n",
    "    tweet_arr=removeStopWords(tweet_arr)\n",
    "    tweet_arr=stem(tweet_arr)\n",
    "    tweets=' '.join(tweet_arr)\n",
    "    if not tweets:\n",
    "        continue\n",
    "    for item in hashtag:\n",
    "        if((item in words) and (item not in filter_hashtag)):\n",
    "            filter_hashtag.append(item)\n",
    "    if(filter_hashtag):\n",
    "#         print(filter_hashtag)\n",
    "        image_path=txt_path.strip().split('social_images')[1][1:]\n",
    "        img_loc=image_path.replace('txt','jpg')\n",
    "        output.write(img_loc+'*')\n",
    "        output.write(tweets+'*')\n",
    "        output.write(' '.join(filter_hashtag)+'\\n')\n",
    "        \n",
    "#     if(num==20):\n",
    "#         break\n",
    "#     num+=1\n",
    "output.close()    \n",
    "print('baseline image tweets data finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
