{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '/home/eric/Documents/Hashtag-recommendation-for-social-images/image_text_hashtagging/src/image_text_classification/'\n",
    "captions_filename = root_path + 'image_text_data.txt'\n",
    "image_path='/home/eric/data/social_images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eric/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from itertools import chain\n",
    "import os\n",
    "import pickle\n",
    "from string import digits\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "start_time = time.monotonic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data ...\n",
      "Loaded 57177 captions\n",
      "dataset/design/2018-12-27_16-58-04_UTC.jpg\n",
      "tbt throwbackthursday art photography gallery ootd floral artofvisuals tb wanderlust interior design classic vintage style\n",
      "ve always had this feeling no matter where am in my life that it either memory or dream best last for at peggy guggenheim former home palazzo venier dei leoni on the grand canal\n"
     ]
    }
   ],
   "source": [
    "print('Loading data ...')\n",
    "data_filename=captions_filename\n",
    "data = pd.read_table(data_filename, sep='*')\n",
    "data = np.asarray(data)\n",
    "np.random.shuffle(data)\n",
    "image_files = data[:, 0]\n",
    "captions = data[:, 2]\n",
    "tweets=data[:,1]\n",
    "number_of_captions = image_files.shape[0]\n",
    "print('Loaded', number_of_captions, 'captions')\n",
    "print(image_files[5])\n",
    "print(captions[5])\n",
    "print(tweets[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_sentence(caption):\n",
    "    incorrect_chars = digits + \";.,'/*?Â¿><:{}[\\]|+\"\n",
    "    char_translator = str.maketrans('', '', incorrect_chars)\n",
    "    quotes_translator = str.maketrans('', '', '\"')\n",
    "    clean_caption = caption.strip().lower()\n",
    "    clean_caption = clean_caption.translate(char_translator)\n",
    "    clean_caption = clean_caption.translate(quotes_translator)\n",
    "    clean_caption = clean_caption.split(' ')\n",
    "    return clean_caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing captions longer than 70 ...\n",
      "Number of files removed: 4\n",
      "Current number of files: 57173\n"
     ]
    }
   ],
   "source": [
    "max_caption_length=70\n",
    "print('Removing captions longer than', max_caption_length, '...')\n",
    "reduced_image_files = []\n",
    "reduced_tweets=[]\n",
    "reduced_captions = []\n",
    "previous_file_size = len(captions)\n",
    "for image_arg, caption in enumerate(captions):\n",
    "    lemmatized_caption = lemmatize_sentence(caption)\n",
    "    if (len(lemmatized_caption) <= max_caption_length):\n",
    "        reduced_captions.append(lemmatized_caption)\n",
    "        reduced_tweets.append(tweets[image_arg])\n",
    "        reduced_image_files.append(image_files[image_arg])\n",
    "captions = reduced_captions\n",
    "tweets=reduced_tweets\n",
    "image_files = reduced_image_files\n",
    "current_file_size = len(captions)\n",
    "file_difference = previous_file_size - current_file_size\n",
    "print('Number of files removed:', file_difference)\n",
    "print('Current number of files:', current_file_size)\n",
    "initial_number_of_captions = previous_file_size\n",
    "number_of_captions_removed = file_difference\n",
    "current_number_of_captions = current_file_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequencies = Counter(chain(*captions)).most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing words with a frequency less than 1 ...\n",
      "Number of words removed: 0\n",
      "Current number of words: 998\n"
     ]
    }
   ],
   "source": [
    "word_frequency_treshold=1\n",
    "#TODO Add option to remove captions that have a words not in vocabulary\n",
    "print('Removing words with a frequency less than',word_frequency_treshold,'...')\n",
    "frequent_threshold_arg=len(word_frequencies)  # set default frequent_threshold_arg\n",
    "for frequency_arg, frequency_data in enumerate(word_frequencies):\n",
    "    frequency = frequency_data[1]\n",
    "    if frequency <= word_frequency_treshold:\n",
    "        frequent_threshold_arg = frequency_arg\n",
    "        break\n",
    "previous_vocabulary_size = len(word_frequencies)\n",
    "if word_frequency_treshold != 0:\n",
    "    word_frequencies = np.asarray(word_frequencies[0:frequent_threshold_arg])\n",
    "else:\n",
    "    word_frequencies = np.asarray(word_frequencies)\n",
    "current_vocabulary_size = word_frequencies.shape[0]\n",
    "vocabulary_difference = (previous_vocabulary_size -\n",
    "                                current_vocabulary_size)\n",
    "print('Number of words removed:',vocabulary_difference)\n",
    "print('Current number of words:',current_vocabulary_size)\n",
    "initial_number_of_words = previous_vocabulary_size\n",
    "number_of_words_removed = vocabulary_difference\n",
    "current_number_of_words = current_vocabulary_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOS = '<S>' #Beginning Of Sentence\n",
    "EOS = '<E>' #End Of Sentence\n",
    "PAD = '<P>'\n",
    "words = word_frequencies[:, 0]\n",
    "word_to_id = {PAD:0, BOS:1, EOS:2}\n",
    "word_to_id.update({word:word_id for word_id, word\n",
    "                                in enumerate(words, 3)})\n",
    "id_to_word = {word_id:word for word, word_id\n",
    "                                in word_to_id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract image features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset/model/2018-11-07_22-20-32_UTC.jpg', 'dataset/pretty/2018-12-23_13-57-00_UTC.jpg', 'dataset/fun/2019-01-02_16-10-00_UTC.jpg', 'dataset/flowers/2018-12-25_09-29-58_UTC.jpg', 'dataset/goodmorning/2019-01-02_18-44-18_UTC.jpg']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57173it [44:50, 21.25it/s]\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.applications import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "\n",
    "IMG_FEATS = 2048\n",
    "image_directory=image_path\n",
    "base_model = InceptionV3(weights='imagenet')\n",
    "model =  Model(inputs=base_model.input,\n",
    "                                outputs=base_model.get_layer('avg_pool').output)\n",
    "extracted_features = []\n",
    "image_feature_files = list(set(image_files))\n",
    "print(image_feature_files[:5])\n",
    "number_of_images = len(image_feature_files)\n",
    "for image_arg,image_file in tqdm(enumerate(image_feature_files)):\n",
    "    _image_path = image_directory + image_file\n",
    "    img = image.load_img(_image_path, target_size=(299, 299))\n",
    "    img = image.img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = preprocess_input(img)\n",
    "    CNN_features = model.predict(img)\n",
    "    extracted_features.append(np.squeeze(CNN_features))\n",
    "extracted_features = np.asarray(extracted_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset/model/2018-11-07_22-20-32_UTC.jpg', 'dataset/pretty/2018-12-23_13-57-00_UTC.jpg', 'dataset/fun/2019-01-02_16-10-00_UTC.jpg', 'dataset/flowers/2018-12-25_09-29-58_UTC.jpg', 'dataset/goodmorning/2019-01-02_18-44-18_UTC.jpg']\n"
     ]
    }
   ],
   "source": [
    "print(image_feature_files[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "470it [00:00, 2348.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing image features to h5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57173it [00:23, 2485.00it/s]\n"
     ]
    }
   ],
   "source": [
    "print('Writing image features to h5...')\n",
    "IMG_FEATS = 2048\n",
    "cnn_extractor='inception'\n",
    "dataset_file = h5py.File(cnn_extractor +'_image_name_to_features.h5')\n",
    "number_of_features = len(image_feature_files)\n",
    "for image_arg, image_file in tqdm(enumerate(image_feature_files)):\n",
    "    file_id = dataset_file.create_group(image_file)\n",
    "    image_data = file_id.create_dataset('image_features',\n",
    "                                        (IMG_FEATS,), dtype='float32')\n",
    "    image_data[:] = extracted_features[image_arg,:]\n",
    "dataset_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = open('complete_data.txt','w')\n",
    "data_file.write('image_names*tweets*hashtags\\n')\n",
    "for image_arg, image_name in enumerate(image_files):\n",
    "    caption = ' '.join(captions[image_arg])\n",
    "    data_file.write('%s*%s*%s\\n' %(image_name,tweets[image_arg],caption))\n",
    "data_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(word_to_id, open('word_to_id.p', 'wb'))\n",
    "pickle.dump(id_to_word, open('id_to_word.p', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "elapsed_time = time.monotonic() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_file = open('data_parameters.log','w')\n",
    "log_file.write('data_filename %s \\n' %data_filename)\n",
    "log_file.write('BOS: %s \\n' % BOS)\n",
    "log_file.write('EOS: %s \\n' % EOS)\n",
    "log_file.write('PAD: %s \\n' % PAD)\n",
    "log_file.write('IMG_FEATS: %s \\n' %IMG_FEATS)\n",
    "log_file.write('word_frequency_threshold: %s \\n'\n",
    "                        %word_frequency_treshold)\n",
    "log_file.write('max_caption_length: %s \\n'\n",
    "                        %max_caption_length)\n",
    "log_file.write('initial_data_size: %s \\n'\n",
    "                        %initial_number_of_captions)\n",
    "log_file.write('captions_larger_than_threshold: %s \\n'\n",
    "                        %number_of_captions_removed)\n",
    "log_file.write('current_data_size: %s \\n'\n",
    "                        %current_number_of_captions)\n",
    "log_file.write('initial_word_size: %s \\n'\n",
    "                        %initial_number_of_words)\n",
    "log_file.write('words_removed_by_frequency_threshold %s \\n'\n",
    "                        %number_of_words_removed)\n",
    "log_file.write('current_word_size: %s \\n'\n",
    "                        %current_number_of_words)\n",
    "log_file.write('cnn_extractor: %s \\n' %cnn_extractor)\n",
    "log_file.write('elapsed_time: %s' %elapsed_time)\n",
    "log_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of training data size: 51455\n",
      "num of validation data size: 5718\n"
     ]
    }
   ],
   "source": [
    "train_porcentage=0.90\n",
    "complete_data = pd.read_table('complete_data.txt',sep='*')\n",
    "data_size = complete_data.shape[0]\n",
    " # training_size = int(data_size*train_porcentage)\n",
    "training_size = int(data_size*1)\n",
    "complete_training_data = complete_data[0:training_size]\n",
    "test_data = complete_data[training_size:]\n",
    "test_data.to_csv('test_data.txt',sep='*',index=False)\n",
    "# splitting between validation and training \n",
    "training_size = int(training_size*train_porcentage)\n",
    "validation_data = complete_training_data[training_size:]\n",
    "training_data = complete_training_data[0:training_size]\n",
    "validation_data.to_csv('validation_data.txt',sep='*',index=False)\n",
    "training_data.to_csv('training_data.txt',sep='*',index=False)\n",
    "print('num of training data size:',training_size)\n",
    "print('num of validation data size:',len(complete_training_data)-training_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
